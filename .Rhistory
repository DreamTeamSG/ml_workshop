# classification learners
fl_classif <- lrn("classif.featureless", predict_type = "prob")
lasso_classif <- lrn("classif.cv_glmnet", nfolds = 5,
predict_type = "prob")
lasso_classif <- as_learner(imputer %>>% lasso_classif)
rf_classif <- lrn("classif.ranger", num.trees = 100,
predict_type = "prob")
rf_classif <- as_learner(imputer %>>% rf_classif)
library(future)
plan("multisession", workers = 2)
set.seed(2)
# regression
design_regr <- benchmark_grid(
tasks = task_Soci,
learners = list(fl_regr, lasso_regr, rf_regr),
resamplings = rsmp("cv", folds = 10))
bm_regr <- benchmark(design_regr)
install.packages("glmnet")
library(glmnet)
# regression
design_regr <- benchmark_grid(
tasks = task_Soci,
learners = list(fl_regr, lasso_regr, rf_regr),
resamplings = rsmp("cv", folds = 10))
bm_regr <- benchmark(design_regr)
# classification
design_classif <- benchmark_grid(
tasks = task_Soci_bin,
learners = list(fl_classif, lasso_classif, rf_classif),
resamplings = rsmp("cv", folds = 10))
task_Soci_bin <- as_task_classif(phonedata,
id = "Sociability_Classif",
target = "E2.Sociableness_bin",
positive = "high")
phonedata$E2.Sociableness_bin <- ifelse(
phonedata$E2.Sociableness >= median(phonedata$E2.Sociableness),
"high", "low")
phonedata$E2.Sociableness_bin <-
as.factor(phonedata$E2.Sociableness_bin)
task_Soci_bin <- as_task_classif(phonedata,
id = "Sociability_Classif",
target = "E2.Sociableness_bin",
positive = "high")
task_Soci_bin$set_col_roles("E2.Sociableness",
remove_from = "feature")
task_Soci_bin$set_col_roles("gender",
remove_from = "feature")
task_Soci_bin
design_classif <- benchmark_grid(
tasks = task_Soci_bin,
learners = list(fl_classif, lasso_classif, rf_classif),
resamplings = rsmp("cv", folds = 10))
bm_classif <- benchmark(design_classif)
plan("sequential")
mes_regr <- msrs(c("regr.rsq", "regr.mse", "regr.srho"))
mes_classif <- msrs(c("classif.ce", "classif.tpr", "classif.tnr"))
# tpr: sensitivity, tnr: specificity
bmr_regr <- bm_regr$aggregate(mes_regr)
bmr_regr[, c(4,7:9)]
plot(bm_regr, measure = msr("regr.rsq"))
bmr_classif <- bm_classif$aggregate(mes_classif)
bmr_classif[, c(4,7:9)]
plot(bm_classif, measure = msr("classif.auc"))
# load packages
library(DALEXtra)
library(ggplot2)
set.seed(123)
rf_regr$train(task_Soci)
rf_exp <- explain_mlr3(rf_regr,
data = phonedata[, c(1:1821,
which(colnames(phonedata) == "gender"))],
y = phonedata$E2.Sociableness,
label = "ranger explainer",
colorize = FALSE)
# select a small subset of variables to reduce compute time
# NOTE: in practice you probably want to use ALL variables!
exemplary_features <- c("nightly_mean_num_call",
"daily_mean_num_call_out",
"daily_mean_num_.com.whatsapp")
varimp <- model_parts(rf_exp, B = 2, N = nrow(phonedata),
variables = exemplary_features,
type = "difference")
plot(varimp, show_boxplots = TRUE)
ice <- model_profile(rf_exp, variables = exemplary_features,
N = 100, center = FALSE, type = "partial")
plot(ice,  geom = "profiles", variables = "nightly_mean_num_call") +
geom_rug(sides = "b") + xlim(0, 2) + ylim(0.5, 2)
pd <- model_profile(rf_exp, variables = exemplary_features,
N = 100, center = FALSE, type = "partial")
plot(pd,  geom = "aggregates", variables = "nightly_mean_num_call") +
geom_rug(sides = "b") + xlim(0, 2) + ylim(0.5, 2)
# Compute a SHAP summary (global) for a manageable subset of observations
# Uses the same explainer 'rf_exp' you created above.
set.seed(123)
# sample a subset to keep it fast during the workshop
shap_idx <- sample(seq_len(nrow(rf_exp$data)), 200)
shap_obs <- rf_exp$data[shap_idx, , drop = FALSE]
# Kernel SHAP via DALEX; B controls the Monte Carlo approximation effort
shap_vals <- predict_parts(
rf_exp,
new_observation = shap_obs,
type = "shap",
B = 25
)
# Compute a SHAP summary (global) for a manageable subset of observations
# Uses the same explainer 'rf_exp' you created above.
set.seed(123)
# sample a subset to keep it fast during the workshop
shap_idx <- sample(seq_len(nrow(rf_exp$data)), 200)
shap_obs <- rf_exp$data[shap_idx, , drop = FALSE]
# Kernel SHAP via DALEX; B controls the Monte Carlo approximation effort
shap_vals <- predict_parts(
rf_exp,
new_observation = shap_obs,
type = "shap",
B = 10
)
## SHAP: Local Explanation for One Participant
# Pick one row from the same subset
one_obs <- shap_obs[1, , drop = FALSE]
shap_one <- predict_parts(
rf_exp,
new_observation = one_obs,
type = "shap",
B = 10
)
varimp
# Compute a SHAP summary (global) for a manageable subset of observations
set.seed(123)
# sample a subset to keep it fast during the workshop
shap_idx <- sample(seq_len(nrow(rf_exp$data)), 200)
shap_obs <- rf_exp$data[shap_idx, , drop = FALSE]
# Kernel SHAP via DALEX; B controls the Monte Carlo approximation effort
shap_vals <- predict_parts(
rf_exp,
variables = exemplary_features,
new_observation = shap_obs,
type = "shap",
B = 10
)
# Compute a SHAP summary (global) for a manageable subset of observations
set.seed(123)
# sample a subset to keep it fast during the workshop
shap_idx <- sample(seq_len(nrow(rf_exp$data)), 100)
shap_obs <- rf_exp$data[shap_idx, , drop = FALSE]
# Kernel SHAP via DALEX; B controls the Monte Carlo approximation effort
shap_vals <- predict_parts(
rf_exp,
variables = exemplary_features,
new_observation = shap_obs,
type = "shap",
B = 5
)
# Compute a SHAP summary (global) for a manageable subset of observations
set.seed(123)
# sample a subset to keep it fast during the workshop
shap_idx <- sample(seq_len(nrow(rf_exp$data)), 10)
shap_obs <- rf_exp$data[shap_idx, , drop = FALSE]
# Kernel SHAP via DALEX; B controls the Monte Carlo approximation effort
shap_vals <- predict_parts(
rf_exp,
variables = exemplary_features,
new_observation = shap_obs,
type = "shap",
B = 5
)
# Compute a SHAP summary (global) for a manageable subset of observations
# set.seed(123)
# # sample a subset to keep it fast during the workshop
# shap_idx <- sample(seq_len(nrow(rf_exp$data)), 10)
# shap_obs <- rf_exp$data[shap_idx, , drop = FALSE]
#
# # Kernel SHAP via DALEX; B controls the Monte Carlo approximation effort
# shap_vals <- predict_parts(
#   rf_exp,
#   variables = exemplary_features,
#   new_observation = shap_obs,
#   type = "shap",
#   B = 5
# )
#
# # Beeswarm-style SHAP summary: variables ordered by overall impact
# plot(shap_vals)
### SHAP (faster without retraining): same model, smaller background
# 1) Small background sample with ALL columns the model expects
set.seed(123)
bg_idx <- sample(seq_len(nrow(phonedata)), 100)
rf_exp_fast <- explain_mlr3(
rf_regr,  # your existing trained model on full features
data   = phonedata[bg_idx, , drop = FALSE],          # all features
y      = phonedata$E2.Sociableness[bg_idx],
label  = "ranger (small background)",
colorize = FALSE
)
# 2) Few observations + small B + restrict to exemplary_features
set.seed(123)
shap_idx <- sample(seq_len(nrow(rf_exp_fast$data)), 10)
shap_obs <- rf_exp_fast$data[shap_idx, , drop = FALSE]
shap_vals <- predict_parts(
rf_exp_fast,
new_observation = shap_obs,
type = "shap",
variables = exemplary_features,   # only compute for these
B = 5
)
# Compute a SHAP summary (global) for a manageable subset of observations
# set.seed(123)
# # sample a subset to keep it fast during the workshop
# shap_idx <- sample(seq_len(nrow(rf_exp$data)), 10)
# shap_obs <- rf_exp$data[shap_idx, , drop = FALSE]
#
# # Kernel SHAP via DALEX; B controls the Monte Carlo approximation effort
# shap_vals <- predict_parts(
#   rf_exp,
#   variables = exemplary_features,
#   new_observation = shap_obs,
#   type = "shap",
#   B = 5
# )
#
# # Beeswarm-style SHAP summary: variables ordered by overall impact
# plot(shap_vals)
### SHAP (fast): retrain on small feature set + small background
# 1) Train a compact RF on the selected features (fast to fit & explain)
task_Soci_small <- task_Soci$clone()
task_Soci_small$select(exemplary_features)
set.seed(123)
rf_regr_small <- lrn("regr.ranger", num.trees = 300, respect.unordered.factors = "order")
rf_regr_small$train(task_Soci_small)
# 2) Build explainer with a SMALL background sample (e.g., 150 rows)
set.seed(123)
bg_idx <- sample(seq_len(nrow(phonedata)), 150)
rf_exp_small <- explain_mlr3(
rf_regr_small,
data   = phonedata[bg_idx, exemplary_features, drop = FALSE],
y      = phonedata$E2.Sociableness[bg_idx],
label  = "ranger (small features + small background)",
colorize = FALSE
)
# 3) Explain only a few observations with small B
set.seed(123)
shap_idx <- sample(seq_len(nrow(rf_exp_small$data)), 20)
shap_obs <- rf_exp_small$data[shap_idx, , drop = FALSE]
shap_vals <- predict_parts(
rf_exp_small,
new_observation = shap_obs,
type = "shap",
variables = exemplary_features,
B = 5
)
plot(shap_vals, max_vars = length(exemplary_features))
# Compute a SHAP summary (global) for a manageable subset of observations
# set.seed(123)
# # sample a subset to keep it fast during the workshop
# shap_idx <- sample(seq_len(nrow(rf_exp$data)), 10)
# shap_obs <- rf_exp$data[shap_idx, , drop = FALSE]
#
# # Kernel SHAP via DALEX; B controls the Monte Carlo approximation effort
# shap_vals <- predict_parts(
#   rf_exp,
#   variables = exemplary_features,
#   new_observation = shap_obs,
#   type = "shap",
#   B = 5
# )
#
# # Beeswarm-style SHAP summary: variables ordered by overall impact
# plot(shap_vals)
### SHAP (fast): retrain on small feature set + small background
# 1) Train a compact RF on the selected features (fast to fit & explain)
task_Soci_small <- task_Soci$clone()
task_Soci_small$select(exemplary_features)
set.seed(123)
rf_regr_small <- lrn("regr.ranger", num.trees = 100, respect.unordered.factors = "order")
rf_regr_small$train(task_Soci_small)
# 2) Build explainer with a SMALL background sample (e.g., 150 rows)
set.seed(123)
rf_exp_small <- explain_mlr3(
rf_regr_small,
data   = phonedata[, exemplary_features, drop = FALSE],
y      = phonedata$E2.Sociableness,
label  = "ranger (small features + small background)",
colorize = FALSE
)
# 3) Explain only a few observations with small B
shap_vals <- predict_parts(
rf_exp_small,
type = "shap",
variables = exemplary_features,
B = 5
)
# Compute a SHAP summary (global) for a manageable subset of observations
# set.seed(123)
# # sample a subset to keep it fast during the workshop
# shap_idx <- sample(seq_len(nrow(rf_exp$data)), 10)
# shap_obs <- rf_exp$data[shap_idx, , drop = FALSE]
#
# # Kernel SHAP via DALEX; B controls the Monte Carlo approximation effort
# shap_vals <- predict_parts(
#   rf_exp,
#   variables = exemplary_features,
#   new_observation = shap_obs,
#   type = "shap",
#   B = 5
# )
#
# # Beeswarm-style SHAP summary: variables ordered by overall impact
# plot(shap_vals)
# --- SHAP with DALEX: all rows, exactly the 3 exemplary variables ---
# 1) Train a compact RF on the selected (3) features
task_Soci_small <- task_Soci$clone()
task_Soci_small$select(exemplary_features)
set.seed(123)
rf_regr_small <- lrn("regr.ranger",
num.trees = 100,
respect.unordered.factors = "order")
rf_regr_small$train(task_Soci_small)
# 2) Build explainer using ALL rows but ONLY the exemplary features
rf_exp_small <- explain_mlr3(
rf_regr_small,
data    = phonedata[, exemplary_features, drop = FALSE],  # all rows, 3 cols
y       = phonedata$E2.Sociableness,
label   = "ranger (3 features, full background)",
colorize = FALSE
)
# 3) SHAP for ALL rows; restrict computation to the 3 variables; keep B small
set.seed(123)
shap_vals <- predict_parts(
rf_exp_small,
new_observation = rf_exp_small$data,     # ALL rows
type       = "shap",
variables  = exemplary_features,         # compute only for the 3 vars
B          = 5
)
# 4) Global SHAP summary (beeswarm)
plot(shap_vals, max_vars = length(exemplary_features))
## SHAP: Local Explanation for One Participant
# Pick one row (e.g., the first) from the explainer data
one_obs <- rf_exp_small$data[1, , drop = FALSE]
# Compute local SHAP for this observation on the 3 exemplary features
set.seed(123)
shap_one <- predict_parts(
rf_exp_small,
new_observation = one_obs,
type      = "shap",
variables = exemplary_features,
B         = 5
)
# Waterfall-style plot showing feature contributions to the prediction
plot(shap_one)
plot(shap_one)
shap_vals
### SHAP — Global Beeswarm (DALEX, 3 features, ALL rows)
# rf_exp_small is your explainer with all rows and only exemplary_features
# rf_exp_small$data has ALL rows, 3 columns (your features)
set.seed(123)
# Compute SHAP per observation (small B to keep it quick) and row-bind
shap_list <- lapply(seq_len(nrow(rf_exp_small$data)), function(i) {
predict_parts(
rf_exp_small,
new_observation = rf_exp_small$data[i, , drop = FALSE],
type      = "shap",
variables = exemplary_features,
B         = 5
)
})
shap_vals_global <- do.call(rbind, shap_list)
# Beeswarm-style global summary (variables ordered by overall impact)
plot(shap_vals_global, max_vars = length(exemplary_features))
install.packages("treeshap")
# install.packages(c("treeshap", "ranger"))
library(ranger)
library(treeshap)
# Train a small Random Forest on your 3 exemplary features
rf_model <- ranger(
formula = E2.Sociableness ~ .,
data    = phonedata[, c(exemplary_features, "E2.Sociableness")],
num.trees = 200,
mtry = 2,
importance = "impurity"
)
# Convert to treeshap-friendly model
unified <- ranger.unify(rf_model, data = phonedata[, exemplary_features])
# Compute SHAP values (exact TreeSHAP)
shap_values <- treeshap(unified, X = phonedata[, exemplary_features])
unified
shap_values <- treeshap(unified, X = phonedata[, exemplary_features])
shap_values <- treeshap(unified, phonedata[, exemplary_features])
plot(shap_values)
install.packages("shapviz")
# Compute a SHAP summary (global) for a manageable subset of observations
# set.seed(123)
# # sample a subset to keep it fast during the workshop
# shap_idx <- sample(seq_len(nrow(rf_exp$data)), 10)
# shap_obs <- rf_exp$data[shap_idx, , drop = FALSE]
#
# # Kernel SHAP via DALEX; B controls the Monte Carlo approximation effort
# shap_vals <- predict_parts(
#   rf_exp,
#   variables = exemplary_features,
#   new_observation = shap_obs,
#   type = "shap",
#   B = 5
# )
#
# # Beeswarm-style SHAP summary: variables ordered by overall impact
# plot(shap_vals)
# # --- SHAP with DALEX: all rows, exactly the 3 exemplary variables ---
#
# # 1) Train a compact RF on the selected (3) features
# task_Soci_small <- task_Soci$clone()
# task_Soci_small$select(exemplary_features)
#
# set.seed(123)
# rf_regr_small <- lrn("regr.ranger",
#                      num.trees = 100,
#                      respect.unordered.factors = "order")
# rf_regr_small$train(task_Soci_small)
#
# # 2) Build explainer using ALL rows but ONLY the exemplary features
# rf_exp_small <- explain_mlr3(
#   rf_regr_small,
#   data    = phonedata[, exemplary_features, drop = FALSE],  # all rows, 3 cols
#   y       = phonedata$E2.Sociableness,
#   label   = "ranger (3 features, full background)",
#   colorize = FALSE
# )
#
# # 3) SHAP for ALL rows; restrict computation to the 3 variables; keep B small
# set.seed(123)
# shap_vals <- predict_parts(
#   rf_exp_small,
#   new_observation = rf_exp_small$data,     # ALL rows
#   type       = "shap",
#   variables  = exemplary_features,         # compute only for the 3 vars
#   B          = 5
# )
#
# # 4) Global SHAP summary (beeswarm)
# plot(shap_vals, max_vars = length(exemplary_features))
# Exact SHAP for tree models + beeswarm via shapviz
# install.packages(c("treeshap", "shapviz"))  # if needed
library(treeshap)
library(shapviz)
library(ranger)
# 1) Fit a compact RF on exactly your 3 features (as you already do)
rf_model <- ranger(
formula   = E2.Sociableness ~ .,
data      = phonedata[, c(exemplary_features, "E2.Sociableness")],
num.trees = 200,
mtry      = min(2, length(exemplary_features)),
importance = "impurity"
)
# 2) Unify model with TRAINING FEATURES ONLY (no target)
unified <- ranger.unify(
rf_model,
data = phonedata[, exemplary_features, drop = FALSE]
)
# 3) Compute exact TreeSHAP for ALL ROWS on the 3 features
shap_values <- treeshap(
unified,
phonedata[, exemplary_features, drop = FALSE]
)
# 4) Visualize: beeswarm with shapviz (global)
sv <- shapviz(shap_values, X = phonedata[, exemplary_features, drop = FALSE])
sv_importance(sv, kind = "beeswarm", max_display = length(exemplary_features))
sv <- shapviz(shap_values, X = phonedata[, exemplary_features, drop = FALSE])
sv_importance(sv, kind = "beeswarm", max_display = length(exemplary_features))
# ## SHAP: Local Explanation for One Participant
#
# # Pick one row (e.g., the first) from the explainer data
# one_obs <- rf_exp_small$data[1, , drop = FALSE]
#
# # Compute local SHAP for this observation on the 3 exemplary features
# set.seed(123)
# shap_one <- predict_parts(
#   rf_exp_small,
#   new_observation = one_obs,
#   type      = "shap",
#   variables = exemplary_features,
#   B         = 5
# )
#
# # Waterfall-style plot showing feature contributions to the prediction
# plot(shap_one)
# Pick any row index you want (e.g., 1)
sv_waterfall(sv, row_index = 1)
sv_waterfall
sv
# ## SHAP: Local Explanation for One Participant
#
# # Pick one row (e.g., the first) from the explainer data
# one_obs <- rf_exp_small$data[1, , drop = FALSE]
#
# # Compute local SHAP for this observation on the 3 exemplary features
# set.seed(123)
# shap_one <- predict_parts(
#   rf_exp_small,
#   new_observation = one_obs,
#   type      = "shap",
#   variables = exemplary_features,
#   B         = 5
# )
#
# # Waterfall-style plot showing feature contributions to the prediction
# plot(shap_one)
# Pick a valid row (avoid NAs)
valid_idx <- which(complete.cases(phonedata[, exemplary_features, drop = FALSE]))[1]
# Local waterfall (prints)
p_wf <- sv_waterfall(sv, row_index = valid_idx, max_display = length(exemplary_features))
print(p_wf)
p_wf
